{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZp4Be3z4eJA"
   },
   "source": [
    "*  DSC 530 Data Exploration and Analysis\n",
    "*  Weeks 1 & 2 Coding Assignment\n",
    "*  Adam Luna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W58yhTdztMCt"
   },
   "source": [
    "# Chapter 3, Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdOIcbCHaBw1"
   },
   "source": [
    "We want to look at data for the Facebook, Apple, Amazon, Netflix, and Google (FAANG) stocks, but we were given each as a separate CSV file (obtained using the stock_analysis package we will build in Chapter 7, Financial Analysis – Bitcoin and the Stock Market). Combine them into a single file and store the dataframe of the FAANG data as faang for the rest of the exercises:\n",
    "\n",
    "- Read in the aapl.csv, amzn.csv, fb.csv, goog.csv, and nflx.csv files.\n",
    "- Add a column to each dataframe, called ticker, indicating the ticker symbol it is for (Apple's is AAPL, for example); this is how you look up a stock. In this case, the filenames happen to be the ticker symbols.\n",
    "- Append them together into a single dataframe.\n",
    "- Save the result in a CSV file called faang.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDv7kr75aBw2"
   },
   "source": [
    "### Reading the stock data\n",
    "\n",
    "In this step, I load the stock price data for each FAANG company from separate CSV files.\n",
    "Each dataset is read into its own dataframe so it can be processed individually before being combined.\n",
    "Because the CSV files are stored in a subdirectory, I specify the appropriate relative file paths\n",
    "when reading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bv2g0VJVaBw2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AAPL_rows': 251,\n",
       " 'AMZN_rows': 251,\n",
       " 'FB_rows': 251,\n",
       " 'GOOG_rows': 251,\n",
       " 'NFLX_rows': 251}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in stock data for each FAANG company\n",
    "aapl = pd.read_csv(\"exercises/aapl.csv\")\n",
    "amzn = pd.read_csv(\"exercises/amzn.csv\")\n",
    "fb   = pd.read_csv(\"exercises/fb.csv\")\n",
    "goog = pd.read_csv(\"exercises/goog.csv\")\n",
    "nflx = pd.read_csv(\"exercises/nflx.csv\")\n",
    "\n",
    "# Verify that each dataset loaded successfully by checking row counts\n",
    "{\n",
    "    \"AAPL_rows\": len(aapl),\n",
    "    \"AMZN_rows\": len(amzn),\n",
    "    \"FB_rows\": len(fb),\n",
    "    \"GOOG_rows\": len(goog),\n",
    "    \"NFLX_rows\": len(nflx)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding ticker identifiers\n",
    "\n",
    "To ensure that each observation can be traced back to its corresponding company after combining\n",
    "the datasets, I add a `ticker` column to each dataframe using the stock’s ticker symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4PN_NYNL5s0g"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>43.075001</td>\n",
       "      <td>42.314999</td>\n",
       "      <td>42.540001</td>\n",
       "      <td>43.064999</td>\n",
       "      <td>102223600.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>43.637501</td>\n",
       "      <td>42.990002</td>\n",
       "      <td>43.132500</td>\n",
       "      <td>43.057499</td>\n",
       "      <td>118071600.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>43.367500</td>\n",
       "      <td>43.020000</td>\n",
       "      <td>43.134998</td>\n",
       "      <td>43.257500</td>\n",
       "      <td>89738400.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>43.842499</td>\n",
       "      <td>43.262501</td>\n",
       "      <td>43.360001</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>94640000.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>43.902500</td>\n",
       "      <td>43.482498</td>\n",
       "      <td>43.587502</td>\n",
       "      <td>43.587502</td>\n",
       "      <td>82271200.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       high        low       open      close       volume ticker\n",
       "0  2018-01-02  43.075001  42.314999  42.540001  43.064999  102223600.0   AAPL\n",
       "1  2018-01-03  43.637501  42.990002  43.132500  43.057499  118071600.0   AAPL\n",
       "2  2018-01-04  43.367500  43.020000  43.134998  43.257500   89738400.0   AAPL\n",
       "3  2018-01-05  43.842499  43.262501  43.360001  43.750000   94640000.0   AAPL\n",
       "4  2018-01-08  43.902500  43.482498  43.587502  43.587502   82271200.0   AAPL"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a ticker column to identify each stock after combining datasets\n",
    "aapl[\"ticker\"] = \"AAPL\"\n",
    "amzn[\"ticker\"] = \"AMZN\"\n",
    "fb[\"ticker\"]   = \"FB\"\n",
    "goog[\"ticker\"] = \"GOOG\"\n",
    "nflx[\"ticker\"] = \"NFLX\"\n",
    "\n",
    "# Confirm that the ticker column was added correctly\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the datasets\n",
    "\n",
    "After adding the ticker identifiers, I append all five datasets into a single dataframe.\n",
    "This unified dataset allows for easier analysis and comparison across FAANG stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker\n",
       "AAPL    251\n",
       "AMZN    251\n",
       "FB      251\n",
       "GOOG    251\n",
       "NFLX    251\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append all five stock datasets into a single FAANG dataframe\n",
    "faang = pd.concat([aapl, amzn, fb, goog, nflx], ignore_index=True)\n",
    "\n",
    "# Inspect the combined dataset to ensure all tickers are present\n",
    "faang[\"ticker\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the combined dataset\n",
    "\n",
    "I save the combined FAANG dataset as a CSV file so it can be reused in subsequent exercises\n",
    "without repeating the data preparation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Kn1IDHUmaBw4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the combined FAANG dataset to a CSV file\n",
    "faang.to_csv(\"faang.csv\", index=False)\n",
    "\n",
    "# Validate that the output file was successfully created\n",
    "import os\n",
    "os.path.exists(\"faang.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn0nOQb_9JDn"
   },
   "source": [
    "# Chapter 3, Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSmLElbyaBw5"
   },
   "source": [
    "With faang, use type conversion to cast the values of the date column into datetimes and the volume column into integers. Then, sort by date and ticker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRVdK9dAaBw5"
   },
   "source": [
    "### Converting data types and sorting the dataset\n",
    "\n",
    "In this step, I convert the `date` column to a datetime type and the `volume` column to integers.\n",
    "After performing these type conversions, I sort the FAANG dataset by date and ticker to ensure\n",
    "the data is in a consistent and logical order for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date      datetime64[ns]\n",
       "high             float64\n",
       "low              float64\n",
       "open             float64\n",
       "close            float64\n",
       "volume             int64\n",
       "ticker            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the date column to datetime format\n",
    "faang[\"date\"] = pd.to_datetime(faang[\"date\"])\n",
    "\n",
    "# Convert the volume column to integer type\n",
    "faang[\"volume\"] = faang[\"volume\"].astype(int)\n",
    "\n",
    "# Sort the dataset by date and ticker\n",
    "faang = faang.sort_values(by=[\"date\", \"ticker\"])\n",
    "\n",
    "# Verify data types and sorting\n",
    "faang.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn0nOQb_9JDn"
   },
   "source": [
    "# Chapter 3, Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the seven rows in faang with the lowest value for volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the lowest trading volumes\n",
    "\n",
    "In this step, I identify the seven observations in the FAANG dataset with the lowest trading\n",
    "volume. This helps highlight periods where trading activity was minimal across the stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>1135.819946</td>\n",
       "      <td>1100.020020</td>\n",
       "      <td>1135.819946</td>\n",
       "      <td>1102.890015</td>\n",
       "      <td>679000</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>1037.589966</td>\n",
       "      <td>1022.398987</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1023.880005</td>\n",
       "      <td>691500</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>2018-05-24</td>\n",
       "      <td>1080.469971</td>\n",
       "      <td>1066.150024</td>\n",
       "      <td>1079.000000</td>\n",
       "      <td>1079.239990</td>\n",
       "      <td>766800</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>2018-07-10</td>\n",
       "      <td>1159.589966</td>\n",
       "      <td>1149.589966</td>\n",
       "      <td>1156.979980</td>\n",
       "      <td>1152.839966</td>\n",
       "      <td>798400</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>2018-08-09</td>\n",
       "      <td>1255.541992</td>\n",
       "      <td>1246.010010</td>\n",
       "      <td>1249.900024</td>\n",
       "      <td>1249.099976</td>\n",
       "      <td>848600</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>2018-08-20</td>\n",
       "      <td>1211.000000</td>\n",
       "      <td>1194.625977</td>\n",
       "      <td>1205.020020</td>\n",
       "      <td>1207.770020</td>\n",
       "      <td>870800</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>1211.839966</td>\n",
       "      <td>1199.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1207.329956</td>\n",
       "      <td>887400</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date         high          low         open        close  volume  \\\n",
       "879 2018-07-03  1135.819946  1100.020020  1135.819946  1102.890015  679000   \n",
       "979 2018-11-23  1037.589966  1022.398987  1030.000000  1023.880005  691500   \n",
       "852 2018-05-24  1080.469971  1066.150024  1079.000000  1079.239990  766800   \n",
       "883 2018-07-10  1159.589966  1149.589966  1156.979980  1152.839966  798400   \n",
       "905 2018-08-09  1255.541992  1246.010010  1249.900024  1249.099976  848600   \n",
       "912 2018-08-20  1211.000000  1194.625977  1205.020020  1207.770020  870800   \n",
       "914 2018-08-22  1211.839966  1199.000000  1200.000000  1207.329956  887400   \n",
       "\n",
       "    ticker  \n",
       "879   GOOG  \n",
       "979   GOOG  \n",
       "852   GOOG  \n",
       "883   GOOG  \n",
       "905   GOOG  \n",
       "912   GOOG  \n",
       "914   GOOG  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the dataset by volume in ascending order\n",
    "lowest_volume = faang.sort_values(by=\"volume\").head(7)\n",
    "\n",
    "# Display the seven rows with the lowest trading volume\n",
    "lowest_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn0nOQb_9JDn"
   },
   "source": [
    "# Chapter 3, Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the data is somewhere between long and wide format. Use melt() to make it completely long format. Hint: date and ticker are our ID variables (they uniquely identify each row). We need to melt the rest so that we don't have separate columns for open, high, low, close, and volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>high</td>\n",
       "      <td>43.075001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>high</td>\n",
       "      <td>1190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>FB</td>\n",
       "      <td>high</td>\n",
       "      <td>181.580002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>high</td>\n",
       "      <td>1066.939941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>high</td>\n",
       "      <td>201.649994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date ticker metric        value\n",
       "0 2018-01-02   AAPL   high    43.075001\n",
       "1 2018-01-02   AMZN   high  1190.000000\n",
       "2 2018-01-02     FB   high   181.580002\n",
       "3 2018-01-02   GOOG   high  1066.939941\n",
       "4 2018-01-02   NFLX   high   201.649994"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the dataset to long format using date and ticker as ID variables\n",
    "faang_long = faang.melt(\n",
    "    id_vars=[\"date\", \"ticker\"],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "# Display the first few rows of the long-format dataset\n",
    "faang_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn0nOQb_9JDn"
   },
   "source": [
    "# Chapter 3, Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we found out that on July 26, 2018 there was a glitch in how the data was recorded. How should we handle this? Note that there is no coding required for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling a data recording glitch\n",
    "\n",
    "If a glitch was discovered in the data recorded on July 26, 2018, the first step would be to\n",
    "recognize that the affected observations may not be reliable. Depending on how widespread or\n",
    "severe the issue appears to be, there are a few reasonable ways to handle the situation.\n",
    "\n",
    "One possible approach would be to remove the affected rows so that incorrect values do not\n",
    "influence the analysis. Another option could be to correct or impute the affected values if\n",
    "there is enough information available to do so in a reasonable way.\n",
    "\n",
    "Overall, the most important consideration is to make sure that the data handling decision is\n",
    "clearly documented. Doing so helps ensure that the results can be interpreted appropriately\n",
    "and that any limitations caused by the data issue are understood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3, Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The European Centre for Disease Prevention and Control (ECDC) provides an open dataset on COVID-19 cases called daily number of new reported cases of COVID-19 by country worldwide. This dataset is updated daily, but we will use a snapshot that contains data from January 1, 2020 through September 18, 2020. Clean and pivot the data so that it is in wide format:\n",
    "\n",
    "- Read in the covid19_cases.csv file.\n",
    "- Create a date column using the data in the dateRep column and the pd.to_datetime() function.\n",
    "- Set the date column as the index and sort the index.\n",
    "- Replace all occurrences of United_States_of_America and United_Kingdom with USA and UK, respectively. Hint: the replace() method can be run on the dataframe as a whole.\n",
    "- Using the countriesAndTerritories column, filter the cleaned COVID-19 cases data down to Argentina, Brazil, China, Colombia, India, Italy, Mexico, Peru, Russia, Spain, Turkey, the UK, and the USA.\n",
    "- Pivot the data so that the index contains the dates, the columns contain the country names, and the values are the case counts (the cases column). Be sure to fill in NaN values with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the COVID-19 cases dataset\n",
    "\n",
    "In this step, I load the COVID-19 cases dataset provided by the European Centre for Disease\n",
    "Prevention and Control. This dataset contains daily reported case counts by country over time\n",
    "and will be cleaned and reshaped for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateRep</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>countriesAndTerritories</th>\n",
       "      <th>geoId</th>\n",
       "      <th>countryterritoryCode</th>\n",
       "      <th>popData2019</th>\n",
       "      <th>continentExp</th>\n",
       "      <th>Cumulative_number_for_14_days_of_COVID-19_cases_per_100000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Lithuania</td>\n",
       "      <td>LT</td>\n",
       "      <td>LTU</td>\n",
       "      <td>2794184.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>IS</td>\n",
       "      <td>ISL</td>\n",
       "      <td>356991.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NP</td>\n",
       "      <td>NPL</td>\n",
       "      <td>28608715.0</td>\n",
       "      <td>Asia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>San_Marino</td>\n",
       "      <td>SM</td>\n",
       "      <td>SMR</td>\n",
       "      <td>34453.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>CA</td>\n",
       "      <td>CAN</td>\n",
       "      <td>37411038.0</td>\n",
       "      <td>America</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dateRep  day  month  year  cases  deaths countriesAndTerritories geoId  \\\n",
       "0  01/01/2020    1      1  2020      0       0               Lithuania    LT   \n",
       "1  01/01/2020    1      1  2020      0       0                 Iceland    IS   \n",
       "2  01/01/2020    1      1  2020      0       0                   Nepal    NP   \n",
       "3  01/01/2020    1      1  2020      0       0              San_Marino    SM   \n",
       "4  01/01/2020    1      1  2020      0       0                  Canada    CA   \n",
       "\n",
       "  countryterritoryCode  popData2019 continentExp  \\\n",
       "0                  LTU    2794184.0       Europe   \n",
       "1                  ISL     356991.0       Europe   \n",
       "2                  NPL   28608715.0         Asia   \n",
       "3                  SMR      34453.0       Europe   \n",
       "4                  CAN   37411038.0      America   \n",
       "\n",
       "   Cumulative_number_for_14_days_of_COVID-19_cases_per_100000  \n",
       "0                                                NaN           \n",
       "1                                                NaN           \n",
       "2                                                NaN           \n",
       "3                                                NaN           \n",
       "4                                                NaN           "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in the COVID-19 cases dataset\n",
    "covid = pd.read_csv(\"exercises/covid19_cases.csv\")\n",
    "\n",
    "# Display the first few rows to confirm the data loaded correctly\n",
    "covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and sorting the date column\n",
    "\n",
    "The dataset includes date information in the `dateRep` column. In this step, I convert that\n",
    "column to a datetime format, create a new `date` column, and then sort the data chronologically\n",
    "to make time-based analysis easier. Because the dates are recorded in day/month/year format,\n",
    "I account for this when parsing the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateRep</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>countriesAndTerritories</th>\n",
       "      <th>geoId</th>\n",
       "      <th>countryterritoryCode</th>\n",
       "      <th>popData2019</th>\n",
       "      <th>continentExp</th>\n",
       "      <th>Cumulative_number_for_14_days_of_COVID-19_cases_per_100000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Lithuania</td>\n",
       "      <td>LT</td>\n",
       "      <td>LTU</td>\n",
       "      <td>2794184.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>IS</td>\n",
       "      <td>ISL</td>\n",
       "      <td>356991.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>NP</td>\n",
       "      <td>NPL</td>\n",
       "      <td>28608715.0</td>\n",
       "      <td>Asia</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>San_Marino</td>\n",
       "      <td>SM</td>\n",
       "      <td>SMR</td>\n",
       "      <td>34453.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>CA</td>\n",
       "      <td>CAN</td>\n",
       "      <td>37411038.0</td>\n",
       "      <td>America</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dateRep  day  month  year  cases  deaths  \\\n",
       "date                                                      \n",
       "2020-01-01  01/01/2020    1      1  2020      0       0   \n",
       "2020-01-01  01/01/2020    1      1  2020      0       0   \n",
       "2020-01-01  01/01/2020    1      1  2020      0       0   \n",
       "2020-01-01  01/01/2020    1      1  2020      0       0   \n",
       "2020-01-01  01/01/2020    1      1  2020      0       0   \n",
       "\n",
       "           countriesAndTerritories geoId countryterritoryCode  popData2019  \\\n",
       "date                                                                         \n",
       "2020-01-01               Lithuania    LT                  LTU    2794184.0   \n",
       "2020-01-01                 Iceland    IS                  ISL     356991.0   \n",
       "2020-01-01                   Nepal    NP                  NPL   28608715.0   \n",
       "2020-01-01              San_Marino    SM                  SMR      34453.0   \n",
       "2020-01-01                  Canada    CA                  CAN   37411038.0   \n",
       "\n",
       "           continentExp  \\\n",
       "date                      \n",
       "2020-01-01       Europe   \n",
       "2020-01-01       Europe   \n",
       "2020-01-01         Asia   \n",
       "2020-01-01       Europe   \n",
       "2020-01-01      America   \n",
       "\n",
       "            Cumulative_number_for_14_days_of_COVID-19_cases_per_100000  \n",
       "date                                                                    \n",
       "2020-01-01                                                NaN           \n",
       "2020-01-01                                                NaN           \n",
       "2020-01-01                                                NaN           \n",
       "2020-01-01                                                NaN           \n",
       "2020-01-01                                                NaN           "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a date column from the dateRep column (day/month/year format)\n",
    "covid[\"date\"] = pd.to_datetime(covid[\"dateRep\"], dayfirst=True)\n",
    "\n",
    "# Set the date column as the index\n",
    "covid = covid.set_index(\"date\")\n",
    "\n",
    "# Sort the dataset by the date index\n",
    "covid = covid.sort_index()\n",
    "\n",
    "covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning country names\n",
    "\n",
    "Some country names in the dataset are written in a longer format. To improve readability,\n",
    "I replace occurrences of `United_States_of_America` and `United_Kingdom` with the shorter\n",
    "labels `USA` and `UK`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace long country names with shorter labels\n",
    "covid = covid.replace({\n",
    "    \"United_States_of_America\": \"USA\",\n",
    "    \"United_Kingdom\": \"UK\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the dataset to selected countries\n",
    "\n",
    "For this analysis, I focus on a subset of countries that represent different regions of the\n",
    "world. I filter the dataset so that it includes only the specified countries before reshaping\n",
    "the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countriesAndTerritories\n",
       "China        262\n",
       "USA          262\n",
       "Spain        262\n",
       "Italy        262\n",
       "Brazil       262\n",
       "Russia       262\n",
       "UK           262\n",
       "India        261\n",
       "Mexico       254\n",
       "Argentina    196\n",
       "Peru         195\n",
       "Colombia     192\n",
       "Turkey       189\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of countries to keep\n",
    "countries = [\n",
    "    \"Argentina\", \"Brazil\", \"China\", \"Colombia\", \"India\", \"Italy\",\n",
    "    \"Mexico\", \"Peru\", \"Russia\", \"Spain\", \"Turkey\", \"UK\", \"USA\"\n",
    "]\n",
    "\n",
    "# Filter the dataset to the selected countries\n",
    "covid_filtered = covid[covid[\"countriesAndTerritories\"].isin(countries)]\n",
    "\n",
    "covid_filtered[\"countriesAndTerritories\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting the data to wide format\n",
    "\n",
    "In this step, I reshape the dataset so that each row represents a date and each column represents\n",
    "a country. The values in the table correspond to the daily number of reported COVID-19 cases.\n",
    "Any missing values are filled with zero to indicate no reported cases on that date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>countriesAndTerritories</th>\n",
       "      <th>Argentina</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>China</th>\n",
       "      <th>Colombia</th>\n",
       "      <th>India</th>\n",
       "      <th>Italy</th>\n",
       "      <th>Mexico</th>\n",
       "      <th>Peru</th>\n",
       "      <th>Russia</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Turkey</th>\n",
       "      <th>UK</th>\n",
       "      <th>USA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "countriesAndTerritories  Argentina  Brazil  China  Colombia  India  Italy  \\\n",
       "date                                                                        \n",
       "2020-01-01                     0.0     0.0    0.0       0.0    0.0    0.0   \n",
       "2020-01-02                     0.0     0.0    0.0       0.0    0.0    0.0   \n",
       "2020-01-03                     0.0     0.0   17.0       0.0    0.0    0.0   \n",
       "2020-01-04                     0.0     0.0    0.0       0.0    0.0    0.0   \n",
       "2020-01-05                     0.0     0.0   15.0       0.0    0.0    0.0   \n",
       "\n",
       "countriesAndTerritories  Mexico  Peru  Russia  Spain  Turkey   UK  USA  \n",
       "date                                                                    \n",
       "2020-01-01                  0.0   0.0     0.0    0.0     0.0  0.0  0.0  \n",
       "2020-01-02                  0.0   0.0     0.0    0.0     0.0  0.0  0.0  \n",
       "2020-01-03                  0.0   0.0     0.0    0.0     0.0  0.0  0.0  \n",
       "2020-01-04                  0.0   0.0     0.0    0.0     0.0  0.0  0.0  \n",
       "2020-01-05                  0.0   0.0     0.0    0.0     0.0  0.0  0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivot the dataset to wide format\n",
    "covid_wide = covid_filtered.pivot(\n",
    "    columns=\"countriesAndTerritories\",\n",
    "    values=\"cases\"\n",
    ")\n",
    "\n",
    "# Replace missing values with zero\n",
    "covid_wide = covid_wide.fillna(0)\n",
    "\n",
    "covid_wide.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
